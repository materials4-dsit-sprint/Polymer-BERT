{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15e31b2e-e13e-4cca-8d29-7f2ef2f17907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import Counter\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d5dcd17-952c-43df-8c32-f74e3d4f767a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d560bd6e-bea2-42a4-91d4-9c5a9181b391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROJECT_ROOT = Path(__file__).resolve().parent\n",
    "PROJECT_ROOT = Path('../')\n",
    "VALIDATION_PATH = PROJECT_ROOT / \"QA-dataset\" / \"autotrain_data\" / \"validation.json\"\n",
    "BERT_MODEL_PATH = PROJECT_ROOT / \"notebooks\" / \"autotrain-bert-ex-qa1\"\n",
    "F_ROBERTA_PATH = PROJECT_ROOT / \"notebooks\" / \"autotrain-roberta-ex-qa1\"\n",
    "ROBERTA_SQUAD2 = \"deepset/roberta-base-squad2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f2c5077-a74b-4d5b-8c1b-de06efca2fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_validation_data(limit: Optional[int] = None) -> list:\n",
    "    \"\"\"Load validation examples from JSON.\"\"\"\n",
    "    with open(VALIDATION_PATH, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    if limit:\n",
    "        data = data[:limit]\n",
    "    return data\n",
    "\n",
    "\n",
    "def normalize_answer(text: str) -> str:\n",
    "    \"\"\"Lower text and remove extra whitespace for fair comparison.\"\"\"\n",
    "    return \" \".join(text.lower().strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f5c4e55-2cb9-494d-a0e1-1eb0f8f2c5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1(prediction: str, ground_truth: str) -> float:\n",
    "    \"\"\"Compute token-level F1 (SQuAD-style) using sklearn. Counter intersection for multiplicity.\"\"\"\n",
    "    pred_tokens = normalize_answer(prediction).split()\n",
    "    truth_tokens = normalize_answer(ground_truth).split()\n",
    "    if not pred_tokens or not truth_tokens:\n",
    "        return 1.0 if pred_tokens == truth_tokens else 0.0\n",
    "    common = Counter(pred_tokens) & Counter(truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0.0\n",
    "    # Build (y_true, y_pred) for sklearn: num_same*(1,1) + (|pred|-num_same)*(0,1) + (|ref|-num_same)*(1,0)\n",
    "    y_true = [1] * num_same + [0] * (len(pred_tokens) - num_same) + [1] * (len(truth_tokens) - num_same)\n",
    "    y_pred = [1] * num_same + [1] * (len(pred_tokens) - num_same) + [0] * (len(truth_tokens) - num_same)\n",
    "    return float(f1_score(y_true, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aa96553-f833-41fd-96b0-bf83ddb6b1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_exact_match(prediction: str, ground_truths: list[str]) -> float:\n",
    "    \"\"\"1.0 if prediction matches any ground truth (normalized), else 0.0.\"\"\"\n",
    "    pred_norm = normalize_answer(prediction)\n",
    "    for gt in ground_truths:\n",
    "        if pred_norm == normalize_answer(gt):\n",
    "            return 1.0\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "089448ef-c82f-439a-91ab-f2cd1671f46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(predictions: list, references: list) -> dict:\n",
    "    \"\"\"Compute exact match and F1 over the dataset.\"\"\"\n",
    "    exact_matches = []\n",
    "    f1_scores = []\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        gt_texts = ref[\"answers\"][\"text\"]\n",
    "        em = max(compute_exact_match(pred, gt_texts), 0.0)\n",
    "        f1 = max(compute_f1(pred, gt) for gt in gt_texts) if gt_texts else 0.0\n",
    "        exact_matches.append(em)\n",
    "        f1_scores.append(f1)\n",
    "    return {\n",
    "        \"exact_match\": 100.0 * sum(exact_matches) / len(exact_matches) if exact_matches else 0.0,\n",
    "        \"f1\": 100.0 * sum(f1_scores) / len(f1_scores) if f1_scores else 0.0,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23bc6405-6ad5-4f5a-819f-1fbcf3756ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bert_hf(\n",
    "    validation: list,\n",
    "    model_path: str,\n",
    "    device: Optional[str] = None,\n",
    ") -> list:\n",
    "    \"\"\"Run a Hugging Face QA model (AutoModelForQuestionAnswering) on validation set.\"\"\"\n",
    "    from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
    "\n",
    "    if device is None:\n",
    "        try:\n",
    "            import torch\n",
    "            if torch.cuda.is_available():\n",
    "                device = 0\n",
    "            elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "                device = \"mps\"\n",
    "            else:\n",
    "                device = -1\n",
    "        except ImportError:\n",
    "            device = -1\n",
    "\n",
    "    qa_pipeline = pipeline(\n",
    "        \"question-answering\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    predictions = []\n",
    "    for item in validation:\n",
    "        result = qa_pipeline(\n",
    "            question=item[\"question\"],\n",
    "            context=item[\"context\"],\n",
    "        )\n",
    "        predictions.append(result[\"answer\"] if result else \"\")\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "861dd829-3edd-4006-b63e-20ace63edbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bert(validation: list, device: Optional[str] = None) -> list:\n",
    "    \"\"\"Run autotrain-bert-ex-qa1 on validation set.\"\"\"\n",
    "    if not BERT_MODEL_PATH.exists():\n",
    "        raise FileNotFoundError(f\"BERT model not found at {BERT_MODEL_PATH}\")\n",
    "    return run_bert_hf(validation, str(BERT_MODEL_PATH), device)\n",
    "\n",
    "def run_f_robert(validation: list, device: Optional[str] = None) -> list:\n",
    "    \"\"\"Run autotrain-roberta-ex-qa1 on validation set.\"\"\"\n",
    "    if not F_ROBERTA_PATH.exists():\n",
    "        raise FileNotFoundError(f\"ROBERTA model not found at {F_ROBERTA_PATH}\")\n",
    "    return run_bert_hf(validation, str(F_ROBERTA_PATH), device)\n",
    "\n",
    "def run_robertasquad2(validation: list, device: Optional[str] = None) -> list:\n",
    "    \"\"\"Run base MatSciBERT from Hugging Face on validation set.\"\"\"\n",
    "    return run_bert_hf(validation, ROBERTA_SQUAD2, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "494961c0-4c3f-4c3a-842e-01b0c57358a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_openai(\n",
    "    validation: list,\n",
    "    model_id: str,\n",
    ") -> list:\n",
    "    \"\"\"Run an OpenAI chat model on extractive QA.\"\"\"\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Please install openai: pip install openai\")\n",
    "\n",
    "    api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"OPENAI_API_KEY environment variable is not set\")\n",
    "\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    predictions = []\n",
    "\n",
    "    for item in validation:\n",
    "        prompt = f\"\"\"You are an extractive question-answering system. Answer the question using ONLY the exact words from the context. Do not paraphrase or add information. If the answer is not in the context, respond with an empty string.\n",
    "\n",
    "                        Context: {item[\"context\"]}\n",
    "                        \n",
    "                        Question: {item[\"question\"]}\n",
    "                        \n",
    "                        Answer (exact extract from context):\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model_id,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0,\n",
    "            )\n",
    "            answer = (response.choices[0].message.content or \"\").strip()\n",
    "            # Remove quotes if model wrapped the answer\n",
    "            if answer.startswith('\"') and answer.endswith('\"'):\n",
    "                answer = answer[1:-1]\n",
    "            predictions.append(answer)\n",
    "        except Exception as e:\n",
    "            print(f\"  [WARN] OpenAI error for {model_id}: {e}\")\n",
    "            predictions.append(\"\")\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6229b18-ddeb-4fcd-bd46-340a5cc371fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ollama (local, free) ---\n",
    "\n",
    "\n",
    "def run_ollama(\n",
    "    validation: list,\n",
    "    model_id: str,\n",
    "    host: str = \"http://localhost:11434\",\n",
    "    ) -> list:\n",
    "    \"\"\"Run an Ollama chat model on extractive QA. Requires Ollama running locally.\"\"\"\n",
    "    try:\n",
    "        from ollama import Client\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Please install ollama: pip install ollama\")\n",
    "\n",
    "    client = Client(host=host)\n",
    "    predictions = []\n",
    "\n",
    "    for item in validation:\n",
    "        prompt = f\"\"\"You are an extractive question-answering system. Answer the question using ONLY the exact words from the context. Do not paraphrase or add information. If the answer is not in the context, respond with an empty string.\n",
    "\n",
    "                Context: {item[\"context\"]}\n",
    "                \n",
    "                Question: {item[\"question\"]}\n",
    "                \n",
    "                Answer (exact extract from context):\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = client.chat(\n",
    "                model=model_id,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                options={\"temperature\": 0, \"num_predict\": 100},\n",
    "            )\n",
    "            answer = (response.message.content or \"\").strip()\n",
    "            if answer.startswith('\"') and answer.endswith('\"'):\n",
    "                answer = answer[1:-1]\n",
    "            predictions.append(answer)\n",
    "        except Exception as e:\n",
    "            print(f\"  [WARN] Ollama error for {model_id}: {e}\")\n",
    "            predictions.append(\"\")\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e5c2623-3ba4-4f6b-b2bd-6679bd0bc66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ids = {\n",
    "        \"bert\": \"autotrain-bert-ex-qa1\",\n",
    "        \"gpt-5.2\": \"gpt-5.2\",\n",
    "        \"gpt-4o\": \"gpt-4o\",\n",
    "        \"qwen2.5\": \"qwen2.5:7b-instruct\",\n",
    "        \"roberta\": \"deepset/roberta-base-squad2\",\n",
    "        \"f-roberta\": \"autotrain-roberta-ex-qa1\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35d70bd9-ef92-4b98-884f-f85ba829be1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading validation data...\n",
      "  Loaded 627 examples\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading validation data...\")\n",
    "validation = load_validation_data()\n",
    "print(f\"  Loaded {len(validation)} examples\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ed97c9c-2c35-49ee-a492-1a2f8607c336",
   "metadata": {},
   "outputs": [],
   "source": [
    "references = [{\"answers\": item[\"answers\"]} for item in validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8eaedd3-8eaf-42f7-a83a-ce68e33b15d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating autotrain-roberta-ex-qa1...\n",
      "  EM: 79.74%  F1: 81.27%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for model_key in [\"bert\", \"gpt-5.2\", \"gpt-4o\", \"qwen2.5\", \"roberta\", \"f-roberta\"]:\n",
    "    display_name = model_ids[model_key]\n",
    "    print(f\"Evaluating {display_name}...\")\n",
    "    try:\n",
    "        if model_key == \"bert\":\n",
    "            predictions = run_bert(validation, device='mps')\n",
    "        elif model_key == \"roberta\":\n",
    "            predictions = run_robertasquad2(validation, device='mps')\n",
    "        elif model_key == \"f-roberta\":\n",
    "            predictions = run_f_robert(validation, device='mps')\n",
    "        elif model_key == \"qwen2.5\":\n",
    "            predictions = run_ollama(validation, model_id=display_name)\n",
    "        else:\n",
    "            predictions = run_openai(validation, model_id=display_name)\n",
    "        metrics = compute_metrics(predictions, references)\n",
    "        results[display_name] = metrics\n",
    "        print(f\"  EM: {metrics['exact_match']:.2f}%  F1: {metrics['f1']:.2f}%\")\n",
    "    except Exception as e:\n",
    "        print(f\"  [ERROR] {e}\")\n",
    "        results[display_name] = {\"error\": str(e)}\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd462e3-9615-436a-b168-faf52b62a76d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
